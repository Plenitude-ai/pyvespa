{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a429e148",
            "metadata": {},
            "source": [
                "<picture>\n",
                "  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://assets.vespa.ai/logos/Vespa-logo-green-RGB.svg\">\n",
                "  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://assets.vespa.ai/logos/Vespa-logo-dark-RGB.svg\">\n",
                "  <img alt=\"#Vespa\" width=\"200\" src=\"https://assets.vespa.ai/logos/Vespa-logo-dark-RGB.svg\" style=\"margin-bottom: 25px;\">\n",
                "</picture>\n",
                "\n",
                "# Multi-vector indexing with HNSW\n",
                "\n",
                "This is the pyvespa steps of the multi-vector-indexing sample application.\n",
                "Go to the [source](https://github.com/vespa-engine/sample-apps/tree/master/multi-vector-indexing)\n",
                "for a full description and prerequisites,\n",
                "and read the [blog post](https://blog.vespa.ai/semantic-search-with-multi-vector-indexing/).\n",
                "Highlighted features:\n",
                "\n",
                "- Approximate Nearest Neighbor Search - using HNSW or exact\n",
                "- Use a Component to configure the Huggingface embedder.\n",
                "- Using synthetic fields with auto-generated\n",
                "  [embeddings](https://docs.vespa.ai/en/embedding.html) in data and query flow.\n",
                "- Application package file export, model files in the application package, deployment from files.\n",
                "- [Multiphased ranking](https://docs.vespa.ai/en/phased-ranking.html).\n",
                "- How to control text search result highlighting.\n",
                "\n",
                "For simpler examples, see [text search](https://vespa-engine.github.io/pyvespa/getting-started-pyvespa.html)\n",
                "and [pyvespa examples](https://vespa-engine.github.io/pyvespa/examples/pyvespa-examples.html).\n",
                "\n",
                "Pyvespa is an add-on to Vespa, and this guide will export the application package containing `services.xml` and `wiki.sd`. The latter is the schema file for this application - knowing services.xml and schema files is useful when reading Vespa documentation.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5f9f78eb",
            "metadata": {},
            "source": [
                "<div class=\"alert alert-info\">\n",
                "    Refer to <a href=\"https://vespa-engine.github.io/pyvespa/troubleshooting.html\">troubleshooting</a>\n",
                "    for any problem when running this guide.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e1fea8a2",
            "metadata": {},
            "source": [
                "This notebook requires [pyvespa >= 0.37.1](https://vespa-engine.github.io/pyvespa/index.html#requirements),\n",
                "ZSTD, and the [Vespa CLI](https://docs.vespa.ai/en/vespa-cli.html).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bf5137e2",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip3 install pyvespa"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0d35f3da",
            "metadata": {},
            "source": [
                "## Create the application\n",
                "\n",
                "Configure the Vespa instance with a component loading the E5-small model.\n",
                "Components are used to plug in code and models to a Vespa application -\n",
                "[read more](https://docs.vespa.ai/en/jdisc/container-components.html):\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "4d7baafb",
            "metadata": {},
            "outputs": [],
            "source": [
                "from vespa.package import (\n",
                "    ApplicationPackage,\n",
                "    Component,\n",
                "    Parameter,\n",
                "    Field,\n",
                "    HNSW,\n",
                "    RankProfile,\n",
                "    Function,\n",
                "    FirstPhaseRanking,\n",
                "    SecondPhaseRanking,\n",
                "    FieldSet,\n",
                "    DocumentSummary,\n",
                "    Summary,\n",
                ")\n",
                "from pathlib import Path\n",
                "import json\n",
                "\n",
                "app_package = ApplicationPackage(\n",
                "    name=\"wiki\",\n",
                "    components=[\n",
                "        Component(\n",
                "            id=\"e5-small-q\",\n",
                "            type=\"hugging-face-embedder\",\n",
                "            parameters=[\n",
                "                Parameter(\"transformer-model\", {\"path\": \"model/e5-small-v2-int8.onnx\"}),\n",
                "                Parameter(\"tokenizer-model\", {\"path\": \"model/tokenizer.json\"}),\n",
                "            ],\n",
                "        )\n",
                "    ],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "feb9a7d9",
            "metadata": {},
            "source": [
                "## Configure fields\n",
                "\n",
                "Vespa has a variety of basic and complex\n",
                "[field types](https://docs.vespa.ai/en/reference/schema-reference.html#field).\n",
                "This application uses a combination of integer, text and tensor fields,\n",
                "making it easy to implement hybrid ranking use cases:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "9106a6a7",
            "metadata": {},
            "outputs": [],
            "source": [
                "app_package.schema.add_fields(\n",
                "    Field(name=\"id\", type=\"int\", indexing=[\"attribute\", \"summary\"]),\n",
                "    Field(\n",
                "        name=\"title\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n",
                "    ),\n",
                "    Field(\n",
                "        name=\"url\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n",
                "    ),\n",
                "    Field(\n",
                "        name=\"paragraphs\",\n",
                "        type=\"array<string>\",\n",
                "        indexing=[\"index\", \"summary\"],\n",
                "        index=\"enable-bm25\",\n",
                "        bolding=True,\n",
                "    ),\n",
                "    Field(\n",
                "        name=\"paragraph_embeddings\",\n",
                "        type=\"tensor<float>(p{},x[384])\",\n",
                "        indexing=[\"input paragraphs\", \"embed\", \"index\", \"attribute\"],\n",
                "        ann=HNSW(distance_metric=\"angular\"),\n",
                "        is_document_field=False,\n",
                "    ),\n",
                "    #\n",
                "    # Alteratively, for exact distance calculation not using HNSW:\n",
                "    #\n",
                "    # Field(name=\"paragraph_embeddings\", type=\"tensor<float>(p{},x[384])\",\n",
                "    #       indexing=[\"input paragraphs\", \"embed\", \"attribute\"],\n",
                "    #       attribute=[\"distance-metric: angular\"],\n",
                "    #       is_document_field=False)\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a88be785",
            "metadata": {},
            "source": [
                "One field of particular interest is `paragraph_embeddings`.\n",
                "Note that we are _not_ feeding embeddings to this instance.\n",
                "Instead, the embeddings are generated by using the [embed](https://docs.vespa.ai/en/embedding.html)\n",
                "feature, using the model configured at start.\n",
                "Read more in [Text embedding made simple](https://blog.vespa.ai/text-embedding-made-simple/).\n",
                "\n",
                "Looking closely at the code, `paragraph_embeddings` uses `is_document_field=False`, meaning it will read another field as input (here `paragraph`), and run `embed` on it.\n",
                "\n",
                "As only one model is configured, `embed` will use that one -\n",
                "it is possible to configure mode models and use `embed model-id` as well.\n",
                "\n",
                "As the code comment illustrates, there can be different distrance metrics used,\n",
                "as well as using an _exact_ or _approximate_ nearest neighbor search.\n",
                "\n",
                "## Configure rank profiles\n",
                "\n",
                "A rank profile defines the computation for the ranking,\n",
                "with a wide range of possible features as input.\n",
                "Below you will find `first_phase` ranking using text ranking (`bm`),\n",
                "semantic ranking using vector distance (consider a tensor a vector here),\n",
                "and combinations of the two:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "d4048480",
            "metadata": {},
            "outputs": [],
            "source": [
                "app_package.schema.add_rank_profile(\n",
                "    RankProfile(\n",
                "        name=\"semantic\",\n",
                "        inputs=[(\"query(q)\", \"tensor<float>(x[384])\")],\n",
                "        inherits=\"default\",\n",
                "        first_phase=\"cos(distance(field,paragraph_embeddings))\",\n",
                "        match_features=[\"closest(paragraph_embeddings)\"],\n",
                "    )\n",
                ")\n",
                "\n",
                "app_package.schema.add_rank_profile(\n",
                "    RankProfile(name=\"bm25\", first_phase=\"2*bm25(title) + bm25(paragraphs)\")\n",
                ")\n",
                "\n",
                "app_package.schema.add_rank_profile(\n",
                "    RankProfile(\n",
                "        name=\"hybrid\",\n",
                "        inherits=\"semantic\",\n",
                "        functions=[\n",
                "            Function(\n",
                "                name=\"avg_paragraph_similarity\",\n",
                "                expression=\"\"\"reduce(\n",
                "                              sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x),\n",
                "                              avg,\n",
                "                              p\n",
                "                          )\"\"\",\n",
                "            ),\n",
                "            Function(\n",
                "                name=\"max_paragraph_similarity\",\n",
                "                expression=\"\"\"reduce(\n",
                "                              sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x),\n",
                "                              max,\n",
                "                              p\n",
                "                          )\"\"\",\n",
                "            ),\n",
                "            Function(\n",
                "                name=\"all_paragraph_similarities\",\n",
                "                expression=\"sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x)\",\n",
                "            ),\n",
                "        ],\n",
                "        first_phase=FirstPhaseRanking(\n",
                "            expression=\"cos(distance(field,paragraph_embeddings))\"\n",
                "        ),\n",
                "        second_phase=SecondPhaseRanking(\n",
                "            expression=\"firstPhase + avg_paragraph_similarity() + log( bm25(title) + bm25(paragraphs) + bm25(url))\"\n",
                "        ),\n",
                "        match_features=[\n",
                "            \"closest(paragraph_embeddings)\",\n",
                "            \"firstPhase\",\n",
                "            \"bm25(title)\",\n",
                "            \"bm25(paragraphs)\",\n",
                "            \"avg_paragraph_similarity\",\n",
                "            \"max_paragraph_similarity\",\n",
                "            \"all_paragraph_similarities\",\n",
                "        ],\n",
                "    )\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4f0048dc",
            "metadata": {},
            "source": [
                "## Configure fieldset\n",
                "\n",
                "A [fieldset](https://docs.vespa.ai/en/reference/schema-reference.html#fieldset)\n",
                "is a way to configure search in multiple fields:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "f9e14283",
            "metadata": {},
            "outputs": [],
            "source": [
                "app_package.schema.add_field_set(\n",
                "    FieldSet(name=\"default\", fields=[\"title\", \"url\", \"paragraphs\"])\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e70a1db4",
            "metadata": {},
            "source": [
                "## Configure document summary\n",
                "\n",
                "A [document summary](https://docs.vespa.ai/en/document-summaries.html)\n",
                "is the collection of fields to return in query results -\n",
                "the default summary is used unless other specified in the query.\n",
                "Here we configure a `minimal` fieldset without the larger paragraph text/embedding fields:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "293dac93",
            "metadata": {},
            "outputs": [],
            "source": [
                "app_package.schema.add_document_summary(\n",
                "    DocumentSummary(\n",
                "        name=\"minimal\",\n",
                "        summary_fields=[Summary(\"id\", \"int\"), Summary(\"title\", \"string\")],\n",
                "    )\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "364c6ef2",
            "metadata": {},
            "source": [
                "## Export the configuration\n",
                "\n",
                "At this point, the application is well defined.\n",
                "Remember that the Component configuration at start configures model files to be found in a `model` directory.\n",
                "We must therefore export the configuration and add the models, before we can deploy to the Vespa instance.\n",
                "Export the [application package](https://docs.vespa.ai/en/application-packages.html):\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "140c9839",
            "metadata": {},
            "outputs": [],
            "source": [
                "Path(\"pkg\").mkdir(parents=True, exist_ok=True)\n",
                "app_package.to_files(\"pkg\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c4febd6b",
            "metadata": {},
            "source": [
                "It is a good idea to inspect the files exported into `pkg` - these are files referred to in the\n",
                "[Vespa Documentation](https://docs.vespa.ai/).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "16f5316d",
            "metadata": {},
            "source": [
                "## Download model files\n",
                "\n",
                "At this point, we can save the model files into the application package:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "32b87bdd",
            "metadata": {
                "scrolled": true,
                "vscode": {
                    "languageId": "shellscript"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
                        "                                 Dload  Upload   Total   Spent    Left  Speed\n",
                        "100  694k  100  694k    0     0  2473k      0 --:--:-- --:--:-- --:--:-- 2508k\n",
                        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
                        "                                 Dload  Upload   Total   Spent    Left  Speed\n",
                        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
                        "100 32.3M  100 32.3M    0     0  27.1M      0  0:00:01  0:00:01 --:--:-- 53.0M\n"
                    ]
                }
            ],
            "source": [
                "! mkdir -p pkg/model\n",
                "! curl -L -o pkg/model/tokenizer.json \\\n",
                "  https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\n",
                "\n",
                "! curl -L -o pkg/model/e5-small-v2-int8.onnx \\\n",
                "  https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a53d5ed0",
            "metadata": {},
            "source": [
                "## Deploy the application\n",
                "\n",
                "As all the files in the app package are ready, we can start a Vespa instance - here using Docker.\n",
                "Deploy the app package:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "02821efd",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Waiting for configuration server, 0/300 seconds...\n",
                        "Waiting for configuration server, 5/300 seconds...\n",
                        "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
                        "Waiting for application status, 0/300 seconds...\n",
                        "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
                        "Waiting for application status, 5/300 seconds...\n",
                        "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
                        "Application is up!\n",
                        "Finished deployment.\n"
                    ]
                }
            ],
            "source": [
                "from vespa.deployment import VespaDocker\n",
                "\n",
                "vespa_docker = VespaDocker()\n",
                "app = vespa_docker.deploy_from_disk(application_name=\"wiki\", application_root=\"pkg\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d2c1060b",
            "metadata": {},
            "source": [
                "## Feed documents\n",
                "\n",
                "Download the Wikipedia articles:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "1a74eff0",
            "metadata": {},
            "outputs": [],
            "source": [
                "! curl -s -H \"Accept:application/vnd.github.v3.raw\" \\\n",
                "  https://api.github.com/repos/vespa-engine/sample-apps/contents/multi-vector-indexing/ext/articles.jsonl.zst | \\\n",
                "  zstdcat - > articles.jsonl"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2d8e0395",
            "metadata": {},
            "source": [
                "I you do not have ZSTD install, get `articles.jsonl.zip` and unzip it instead.\n",
                "\n",
                "Feed and index the Wikipedia articles using the [Vespa CLI](https://docs.vespa.ai/en/vespa-cli.html).\n",
                "As part of feeding, `embed` is called on each article,\n",
                "and the output of this is stored in the `paragraph_embeddings` field:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "8130fdfa",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{\n",
                        "  \"feeder.seconds\": 1.448,\n",
                        "  \"feeder.ok.count\": 8,\n",
                        "  \"feeder.ok.rate\": 5.524,\n",
                        "  \"feeder.error.count\": 0,\n",
                        "  \"feeder.inflight.count\": 0,\n",
                        "  \"http.request.count\": 8,\n",
                        "  \"http.request.bytes\": 12958,\n",
                        "  \"http.request.MBps\": 0.009,\n",
                        "  \"http.exception.count\": 0,\n",
                        "  \"http.response.count\": 8,\n",
                        "  \"http.response.bytes\": 674,\n",
                        "  \"http.response.MBps\": 0.000,\n",
                        "  \"http.response.error.count\": 0,\n",
                        "  \"http.response.latency.millis.min\": 728,\n",
                        "  \"http.response.latency.millis.avg\": 834,\n",
                        "  \"http.response.latency.millis.max\": 1446,\n",
                        "  \"http.response.code.counts\": {\n",
                        "    \"200\": 8\n",
                        "  }\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "! vespa config set target local\n",
                "! vespa feed articles.jsonl"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a7461f9a",
            "metadata": {},
            "source": [
                "Note that creating embeddings is computationally expensive, but this is a small dataset with only 8 articles, so will be done in a few seconds.\n",
                "\n",
                "The Vespa instance is now populated with the Wikipedia articles, with generated embeddings, and ready for queries.\n",
                "The next sections have examples of various kinds of queries to run on the dataset.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c3b9e58c",
            "metadata": {},
            "source": [
                "## Simple retrieve all articles with undefined ranking\n",
                "\n",
                "Run a query selecting _all_ documents, returning two of them.\n",
                "The rank profile is the built-in `unranked` which means no ranking calculations are done,\n",
                "the results are returned in random order:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "06dd6e2a",
            "metadata": {},
            "outputs": [],
            "source": [
                "from vespa.io import VespaQueryResponse\n",
                "\n",
                "result: VespaQueryResponse = app.query(\n",
                "    body={\n",
                "        \"yql\": \"select * from wiki where true\",\n",
                "        \"ranking.profile\": \"unranked\",\n",
                "        \"hits\": 2,\n",
                "    }\n",
                ")\n",
                "if not result.is_successful():\n",
                "    raise ValueError(result.get_json())\n",
                "if len(result.hits) != 2:\n",
                "    raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits)))\n",
                "print(json.dumps(result.hits, indent=4))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "72937dc2",
            "metadata": {},
            "source": [
                "## Traditional keyword search with BM25 ranking on the article level\n",
                "\n",
                "Run a text-search query and use the [bm25](https://docs.vespa.ai/en/reference/bm25.html)\n",
                "ranking profile configured at the start of this guide: `2*bm25(title) + bm25(paragraphs)`.\n",
                "Here, we use BM25 on the `title` and `paragraph` text fields, giving more weight to matches in title:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0138aa1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "result = app.query(\n",
                "    body={\n",
                "        \"yql\": \"select * from wiki where userQuery()\",\n",
                "        \"query\": 24,\n",
                "        \"ranking.profile\": \"bm25\",\n",
                "        \"hits\": 2,\n",
                "    }\n",
                ")\n",
                "if len(result.hits) != 2:\n",
                "    raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits)))\n",
                "print(json.dumps(result.hits, indent=4))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a716cb42",
            "metadata": {},
            "source": [
                "## Semantic vector search on the paragraph level\n",
                "\n",
                "This query creates an embedding of the query \"what does 24 mean in the context of railways\"\n",
                "and specifies the `semantic` ranking profile: `cos(distance(field,paragraph_embeddings))`.\n",
                "This will hence compute the distance between the vector in the query\n",
                "and the vectors computed when indexing: `\"input paragraphs\", \"embed\", \"index\", \"attribute\"`:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "3bccdb4c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[\n",
                        "    {\n",
                        "        \"id\": \"id:wikipedia:wiki::9985\",\n",
                        "        \"relevance\": 0.8807156260391702,\n",
                        "        \"source\": \"wiki_content\",\n",
                        "        \"fields\": {\n",
                        "            \"matchfeatures\": {\n",
                        "                \"closest(paragraph_embeddings)\": {\n",
                        "                    \"4\": 1.0\n",
                        "                }\n",
                        "            },\n",
                        "            \"sddocname\": \"wiki\",\n",
                        "            \"paragraphs\": [\n",
                        "                \"The 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and is divided into 24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only in the US and the English speaking parts of Canada) as military time or (only in the United Kingdom and now very rarely) as continental time. In some parts of the world, it is called railway time. Also, the international standard notation of time (ISO 8601) is based on this format.\",\n",
                        "                \"A time in the 24-hour clock is written in the form hours:minutes (for example, 01:23), or hours:minutes:seconds (01:23:45). Numbers under 10 have a zero in front (called a leading zero); e.g. 09:07. Under the 24-hour clock system, the day begins at midnight, 00:00, and the last minute of the day begins at 23:59 and ends at 24:00, which is identical to 00:00 of the following day. 12:00 can only be mid-day. Midnight is called 24:00 and is used to mean the end of the day and 00:00 is used to mean the beginning of the day. For example, you would say \\\"Tuesday at 24:00\\\" and \\\"Wednesday at 00:00\\\" to mean exactly the same time.\",\n",
                        "                \"However, the US military prefers not to say 24:00 - they do not like to have two names for the same thing, so they always say \\\"23:59\\\", which is one minute before midnight.\",\n",
                        "                \"24-hour clock time is used in computers, military, public safety, and transport. In many Asian, European and Latin American countries people use it to write the time. Many European people use it in speaking.\",\n",
                        "                \"In railway timetables 24:00 means the \\\"end\\\" of the day. For example, a train due to arrive at a station during the last minute of a day arrives at 24:00; but trains which depart during the first minute of the day go at 00:00.\"\n",
                        "            ],\n",
                        "            \"documentid\": \"id:wikipedia:wiki::9985\",\n",
                        "            \"title\": \"24-hour clock\",\n",
                        "            \"url\": \"https://simple.wikipedia.org/wiki?curid=9985\"\n",
                        "        }\n",
                        "    },\n",
                        "    {\n",
                        "        \"id\": \"id:wikipedia:wiki::59079\",\n",
                        "        \"relevance\": 0.7972394509946005,\n",
                        "        \"source\": \"wiki_content\",\n",
                        "        \"fields\": {\n",
                        "            \"matchfeatures\": {\n",
                        "                \"closest(paragraph_embeddings)\": {\n",
                        "                    \"4\": 1.0\n",
                        "                }\n",
                        "            },\n",
                        "            \"sddocname\": \"wiki\",\n",
                        "            \"paragraphs\": [\n",
                        "                \"Logic gates are digital components. They normally work at only two levels of voltage, a positive level and zero level. Commonly they work based on two states: \\\"On\\\" and \\\"Off\\\". In the On state, voltage is positive. In the Off state, the voltage is at zero. The On state usually uses a voltage in the range of 3.5 to 5 volts. This range can be lower for some uses.\",\n",
                        "                \"Logic gates compare the state at their inputs to decide what the state at their output should be. A logic gate is \\\"on\\\" or active when its rules are correctly met. At this time, electricity is flowing through the gate and the voltage at its output is at the level of its On state.\",\n",
                        "                \"Logic gates are electronic versions of Boolean logic. Truth tables will tell you what the output will be, depending on the inputs.\",\n",
                        "                \"AND gates have two inputs. The output of an AND gate is on only if both inputs are on. If at least one of the inputs is off, the output will be off.\",\n",
                        "                \"Using the image at the right, if \\\"A\\\" and \\\"B\\\" are both in an On state, the output (out) will be an On state. If either \\\"A\\\" or \\\"B\\\" is in an Off state, the output will also be in an Off state. \\\"A\\\" and \\\"B\\\" must be On for the output to be On.\",\n",
                        "                \"OR gates have two inputs. The output of an OR gate will be on if at least one of the inputs are on. If both inputs are off, the output will be off.\",\n",
                        "                \"Using the image at the right, if either \\\"A\\\" or \\\"B\\\" is On, the output (\\\"out\\\") will also be On. If both \\\"A\\\" and \\\"B\\\" are Off, the output will be Off.\",\n",
                        "                \"The NOT logic gate has only one input. If the input is On then the output will be Off. In other words, the NOT logic gate changes the signal from On to Off or from Off to On. It is sometimes called an inverter.\",\n",
                        "                \"XOR (\\\"exclusive or\\\") gates have two inputs. The output of a XOR gate will be true only if the two inputs are different from each other. If both inputs are the same, the output will be off.\",\n",
                        "                \"NAND means not both. It is called NAND because it means \\\"not and.\\\" This means that it will always output true unless both inputs are on.\",\n",
                        "                \"XNOR means \\\"not exclusive or.\\\" This means that it will only output true if both inputs are the same. It is the opposite of a XOR logic gate.\"\n",
                        "            ],\n",
                        "            \"documentid\": \"id:wikipedia:wiki::59079\",\n",
                        "            \"title\": \"Logic gate\",\n",
                        "            \"url\": \"https://simple.wikipedia.org/wiki?curid=59079\"\n",
                        "        }\n",
                        "    }\n",
                        "]\n"
                    ]
                }
            ],
            "source": [
                "result = app.query(\n",
                "    body={\n",
                "        \"yql\": \"select * from wiki where {targetHits:2}nearestNeighbor(paragraph_embeddings,q)\",\n",
                "        \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",\n",
                "        \"ranking.profile\": \"semantic\",\n",
                "        \"presentation.format.tensors\": \"short-value\",\n",
                "        \"hits\": 2,\n",
                "    }\n",
                ")\n",
                "result.hits\n",
                "if len(result.hits) != 2:\n",
                "    raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits)))\n",
                "print(json.dumps(result.hits, indent=4))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c134faa6",
            "metadata": {},
            "source": [
                "An interesting question then is, of the paragraphs in the document, which one was the closest?\n",
                "When analysing ranking,\n",
                "using [match-features](https://docs.vespa.ai/en/reference/schema-reference.html#match-features)\n",
                "lets you export the scores used in the ranking calculations, see\n",
                "[closest](<https://docs.vespa.ai/en/reference/rank-features.html#closest(name)>) - from the result above:\n",
                "\n",
                "```\n",
                " \"matchfeatures\": {\n",
                "                \"closest(paragraph_embeddings)\": {\n",
                "                    \"4\": 1.0\n",
                "                }\n",
                "}\n",
                "```\n",
                "\n",
                "This means, the tensor of index 4 has the closest match. With this, it is straight forward to feed articles with an array of paragraphs and highlight the best matching paragraph in the document!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "18875db4",
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_best_paragraph(hit: dict) -> str:\n",
                "    paragraphs = hit[\"fields\"][\"paragraphs\"]\n",
                "    match_features = hit[\"fields\"][\"matchfeatures\"]\n",
                "    index = int(list(match_features[\"closest(paragraph_embeddings)\"].keys())[0])\n",
                "    return paragraphs[index]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "6a8738a6",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'In railway timetables 24:00 means the \"end\" of the day. For example, a train due to arrive at a station during the last minute of a day arrives at 24:00; but trains which depart during the first minute of the day go at 00:00.'"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "find_best_paragraph(result.hits[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "15c9fc99",
            "metadata": {},
            "source": [
                "## Hybrid search and ranking\n",
                "\n",
                "Hybrid combining keyword search on the article level with vector search in the paragraph index:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "4ad0d11f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[\n",
                        "    {\n",
                        "        \"id\": \"id:wikipedia:wiki::9985\",\n",
                        "        \"relevance\": 4.163399168193791,\n",
                        "        \"source\": \"wiki_content\",\n",
                        "        \"fields\": {\n",
                        "            \"matchfeatures\": {\n",
                        "                \"bm25(paragraphs)\": 10.468827250036052,\n",
                        "                \"bm25(title)\": 1.1272217840066168,\n",
                        "                \"closest(paragraph_embeddings)\": {\n",
                        "                    \"4\": 1.0\n",
                        "                },\n",
                        "                \"firstPhase\": 0.8807156260391702,\n",
                        "                \"all_paragraph_similarities\": {\n",
                        "                    \"1\": 0.8030083179473877,\n",
                        "                    \"2\": 0.7992785573005676,\n",
                        "                    \"3\": 0.8273358345031738,\n",
                        "                    \"4\": 0.8807156085968018,\n",
                        "                    \"0\": 0.849757194519043\n",
                        "                },\n",
                        "                \"avg_paragraph_similarity\": 0.8320191025733947,\n",
                        "                \"max_paragraph_similarity\": 0.8807156085968018\n",
                        "            },\n",
                        "            \"sddocname\": \"wiki\",\n",
                        "            \"paragraphs\": [\n",
                        "                \"<hi>The</hi> <hi>24</hi>-hour clock is a way <hi>of</hi> telling <hi>the</hi> time <hi>in</hi> which <hi>the</hi> day runs from midnight to midnight and is divided into <hi>24</hi> hours, numbered from 0 to 23. It <hi>does</hi> not use a.m. or p.m. This system is also referred to (only <hi>in</hi> <hi>the</hi> US and <hi>the</hi> English speaking parts <hi>of</hi> Canada) as military time or (only <hi>in</hi> <hi>the</hi> United Kingdom and now very rarely) as continental time. <hi>In</hi> some parts <hi>of</hi> <hi>the</hi> world, it is called <hi>railway</hi> time. Also, <hi>the</hi> international standard notation <hi>of</hi> time (ISO 8601) is based on this format.\",\n",
                        "                \"A time <hi>in</hi> <hi>the</hi> <hi>24</hi>-hour clock is written <hi>in</hi> <hi>the</hi> form hours:minutes (for example, 01:23), or hours:minutes:seconds (01:23:45). Numbers under 10 have a zero <hi>in</hi> front (called a leading zero); e.g. 09:07. Under <hi>the</hi> <hi>24</hi>-hour clock system, <hi>the</hi> day begins at midnight, 00:00, and <hi>the</hi> last minute <hi>of</hi> <hi>the</hi> day begins at 23:59 and ends at <hi>24</hi>:00, which is identical to 00:00 <hi>of</hi> <hi>the</hi> following day. 12:00 can only be mid-day. Midnight is called <hi>24</hi>:00 and is used to <hi>mean</hi> <hi>the</hi> end <hi>of</hi> <hi>the</hi> day and 00:00 is used to <hi>mean</hi> <hi>the</hi> beginning <hi>of</hi> <hi>the</hi> day. For example, you would say \\\"Tuesday at <hi>24</hi>:00\\\" and \\\"Wednesday at 00:00\\\" to <hi>mean</hi> exactly <hi>the</hi> same time.\",\n",
                        "                \"However, <hi>the</hi> US military prefers not to say <hi>24</hi>:00 - they <hi>do</hi> not like to have two names for <hi>the</hi> same thing, so they always say \\\"23:59\\\", which is one minute before midnight.\",\n",
                        "                \"<hi>24</hi>-hour clock time is used <hi>in</hi> computers, military, public safety, and transport. <hi>In</hi> many Asian, European and Latin American countries people use it to write <hi>the</hi> time. Many European people use it <hi>in</hi> speaking.\",\n",
                        "                \"<hi>In</hi> <hi>railway</hi> timetables <hi>24</hi>:00 means <hi>the</hi> \\\"end\\\" <hi>of</hi> <hi>the</hi> day. For example, a train due to arrive at a station during <hi>the</hi> last minute <hi>of</hi> a day arrives at <hi>24</hi>:00; but trains which depart during <hi>the</hi> first minute <hi>of</hi> <hi>the</hi> day go at 00:00.\"\n",
                        "            ],\n",
                        "            \"documentid\": \"id:wikipedia:wiki::9985\",\n",
                        "            \"title\": \"24-hour clock\",\n",
                        "            \"url\": \"https://simple.wikipedia.org/wiki?curid=9985\"\n",
                        "        }\n",
                        "    }\n",
                        "]\n"
                    ]
                }
            ],
            "source": [
                "result = app.query(\n",
                "    body={\n",
                "        \"yql\": \"select * from wiki where userQuery() or ({targetHits:1}nearestNeighbor(paragraph_embeddings,q))\",\n",
                "        \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",\n",
                "        \"query\": \"what does 24 mean in the context of railways\",\n",
                "        \"ranking.profile\": \"hybrid\",\n",
                "        \"presentation.format.tensors\": \"short-value\",\n",
                "        \"hits\": 1,\n",
                "    }\n",
                ")\n",
                "if len(result.hits) != 1:\n",
                "    raise ValueError(\"Expected 1 hits, got {}\".format(len(result.hits)))\n",
                "print(json.dumps(result.hits, indent=4))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4a6dd1f4",
            "metadata": {},
            "source": [
                "This case combines exact search with nearestNeighbor search. The `hybrid` rank-profile above\n",
                "also calculates several additional features using\n",
                "[tensor expressions](https://docs.vespa.ai/en/tensor-user-guide.html):\n",
                "\n",
                "- `firstPhase` is the score of the first ranking phase, configured in the hybrid\n",
                "  profile as `cos(distance(field, paragraph_embeddings))`.\n",
                "- `all_paragraph_similarities` returns all the similarity scores for all paragraphs.\n",
                "- `avg_paragraph_similarity` is the average similarity score across all the paragraphs.\n",
                "- `max_paragraph_similarity` is the same as `firstPhase`, but computed using a tensor expression.\n",
                "\n",
                "These additional features are calculated during [second-phase ranking](https://docs.vespa.ai/en/phased-ranking.html)\n",
                "to limit the number of vector computations.\n",
                "\n",
                "The [Tensor Playground](https://docs.vespa.ai/playground/) is useful to play with tensor expressions.\n",
                "\n",
                "The [Hybrid Search](https://blog.vespa.ai/improving-zero-shot-ranking-with-vespa/) blog post series\n",
                "is a good read to learn more about hybrid ranking!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "6b22aec3",
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_paragraph_scores(hit: dict) -> str:\n",
                "    paragraphs = hit[\"fields\"][\"paragraphs\"]\n",
                "    match_features = hit[\"fields\"][\"matchfeatures\"]\n",
                "    indexes = [int(v) for v in match_features[\"all_paragraph_similarities\"]]\n",
                "    scores = list(match_features[\"all_paragraph_similarities\"].values())\n",
                "    return list(zip([paragraphs[i] for i in indexes], scores))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "ff41f382",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[('A time <hi>in</hi> <hi>the</hi> <hi>24</hi>-hour clock is written <hi>in</hi> <hi>the</hi> form hours:minutes (for example, 01:23), or hours:minutes:seconds (01:23:45). Numbers under 10 have a zero <hi>in</hi> front (called a leading zero); e.g. 09:07. Under <hi>the</hi> <hi>24</hi>-hour clock system, <hi>the</hi> day begins at midnight, 00:00, and <hi>the</hi> last minute <hi>of</hi> <hi>the</hi> day begins at 23:59 and ends at <hi>24</hi>:00, which is identical to 00:00 <hi>of</hi> <hi>the</hi> following day. 12:00 can only be mid-day. Midnight is called <hi>24</hi>:00 and is used to <hi>mean</hi> <hi>the</hi> end <hi>of</hi> <hi>the</hi> day and 00:00 is used to <hi>mean</hi> <hi>the</hi> beginning <hi>of</hi> <hi>the</hi> day. For example, you would say \"Tuesday at <hi>24</hi>:00\" and \"Wednesday at 00:00\" to <hi>mean</hi> exactly <hi>the</hi> same time.',\n",
                            "  0.8030083179473877),\n",
                            " ('However, <hi>the</hi> US military prefers not to say <hi>24</hi>:00 - they <hi>do</hi> not like to have two names for <hi>the</hi> same thing, so they always say \"23:59\", which is one minute before midnight.',\n",
                            "  0.7992785573005676),\n",
                            " ('<hi>24</hi>-hour clock time is used <hi>in</hi> computers, military, public safety, and transport. <hi>In</hi> many Asian, European and Latin American countries people use it to write <hi>the</hi> time. Many European people use it <hi>in</hi> speaking.',\n",
                            "  0.8273358345031738),\n",
                            " ('<hi>In</hi> <hi>railway</hi> timetables <hi>24</hi>:00 means <hi>the</hi> \"end\" <hi>of</hi> <hi>the</hi> day. For example, a train due to arrive at a station during <hi>the</hi> last minute <hi>of</hi> a day arrives at <hi>24</hi>:00; but trains which depart during <hi>the</hi> first minute <hi>of</hi> <hi>the</hi> day go at 00:00.',\n",
                            "  0.8807156085968018),\n",
                            " ('<hi>The</hi> <hi>24</hi>-hour clock is a way <hi>of</hi> telling <hi>the</hi> time <hi>in</hi> which <hi>the</hi> day runs from midnight to midnight and is divided into <hi>24</hi> hours, numbered from 0 to 23. It <hi>does</hi> not use a.m. or p.m. This system is also referred to (only <hi>in</hi> <hi>the</hi> US and <hi>the</hi> English speaking parts <hi>of</hi> Canada) as military time or (only <hi>in</hi> <hi>the</hi> United Kingdom and now very rarely) as continental time. <hi>In</hi> some parts <hi>of</hi> <hi>the</hi> world, it is called <hi>railway</hi> time. Also, <hi>the</hi> international standard notation <hi>of</hi> time (ISO 8601) is based on this format.',\n",
                            "  0.849757194519043)]"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "find_paragraph_scores(result.hits[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d1aa9e8d",
            "metadata": {},
            "source": [
                "## Hybrid search and filter\n",
                "\n",
                "YQL is a structured query langauge.\n",
                "In the query examples, the user input is fed as-is using the `userQuery()` operator.\n",
                "\n",
                "Filters are normally separate from the user input,\n",
                "below is an example of adding a filter `url contains \"9985\"` to the YQL string.\n",
                "\n",
                "Finally, the use the [Query API](https://docs.vespa.ai/en/query-api.html) for other options, like highlighting -\n",
                "here disable [bolding](https://docs.vespa.ai/en/reference/schema-reference.html#bolding):\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "250f2c1b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[\n",
                        "    {\n",
                        "        \"id\": \"id:wikipedia:wiki::9985\",\n",
                        "        \"relevance\": 4.307079208249452,\n",
                        "        \"source\": \"wiki_content\",\n",
                        "        \"fields\": {\n",
                        "            \"matchfeatures\": {\n",
                        "                \"bm25(paragraphs)\": 10.468827250036052,\n",
                        "                \"bm25(title)\": 1.1272217840066168,\n",
                        "                \"closest(paragraph_embeddings)\": {\n",
                        "                    \"type\": \"tensor<float>(p{})\",\n",
                        "                    \"cells\": {\n",
                        "                        \"4\": 1.0\n",
                        "                    }\n",
                        "                },\n",
                        "                \"firstPhase\": 0.8807156260391702,\n",
                        "                \"all_paragraph_similarities\": {\n",
                        "                    \"type\": \"tensor<float>(p{})\",\n",
                        "                    \"cells\": {\n",
                        "                        \"1\": 0.8030083179473877,\n",
                        "                        \"2\": 0.7992785573005676,\n",
                        "                        \"3\": 0.8273358345031738,\n",
                        "                        \"4\": 0.8807156085968018,\n",
                        "                        \"0\": 0.849757194519043\n",
                        "                    }\n",
                        "                },\n",
                        "                \"avg_paragraph_similarity\": 0.8320191025733947,\n",
                        "                \"max_paragraph_similarity\": 0.8807156085968018\n",
                        "            },\n",
                        "            \"sddocname\": \"wiki\",\n",
                        "            \"paragraphs\": [\n",
                        "                \"The 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and is divided into 24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only in the US and the English speaking parts of Canada) as military time or (only in the United Kingdom and now very rarely) as continental time. In some parts of the world, it is called railway time. Also, the international standard notation of time (ISO 8601) is based on this format.\",\n",
                        "                \"A time in the 24-hour clock is written in the form hours:minutes (for example, 01:23), or hours:minutes:seconds (01:23:45). Numbers under 10 have a zero in front (called a leading zero); e.g. 09:07. Under the 24-hour clock system, the day begins at midnight, 00:00, and the last minute of the day begins at 23:59 and ends at 24:00, which is identical to 00:00 of the following day. 12:00 can only be mid-day. Midnight is called 24:00 and is used to mean the end of the day and 00:00 is used to mean the beginning of the day. For example, you would say \\\"Tuesday at 24:00\\\" and \\\"Wednesday at 00:00\\\" to mean exactly the same time.\",\n",
                        "                \"However, the US military prefers not to say 24:00 - they do not like to have two names for the same thing, so they always say \\\"23:59\\\", which is one minute before midnight.\",\n",
                        "                \"24-hour clock time is used in computers, military, public safety, and transport. In many Asian, European and Latin American countries people use it to write the time. Many European people use it in speaking.\",\n",
                        "                \"In railway timetables 24:00 means the \\\"end\\\" of the day. For example, a train due to arrive at a station during the last minute of a day arrives at 24:00; but trains which depart during the first minute of the day go at 00:00.\"\n",
                        "            ],\n",
                        "            \"documentid\": \"id:wikipedia:wiki::9985\",\n",
                        "            \"title\": \"24-hour clock\",\n",
                        "            \"url\": \"https://simple.wikipedia.org/wiki?curid=9985\"\n",
                        "        }\n",
                        "    }\n",
                        "]\n"
                    ]
                }
            ],
            "source": [
                "result = app.query(\n",
                "    body={\n",
                "        \"yql\": 'select * from wiki where url contains \"9985\" and userQuery() or ({targetHits:1}nearestNeighbor(paragraph_embeddings,q))',\n",
                "        \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",\n",
                "        \"query\": \"what does 24 mean in the context of railways\",\n",
                "        \"ranking.profile\": \"hybrid\",\n",
                "        \"bolding\": False,\n",
                "        \"presentation.format.tensors\": \"short-value\",\n",
                "        \"hits\": 1,\n",
                "    }\n",
                ")\n",
                "if len(result.hits) != 1:\n",
                "    raise ValueError(\"Expected one hit, got {}\".format(len(result.hits)))\n",
                "print(json.dumps(result.hits, indent=4))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0a6558f3",
            "metadata": {},
            "source": [
                "In short, the above query demonstrates how easy it is to combine various ranking strategies,\n",
                "and also combine with filters.\n",
                "\n",
                "To learn more about pre-filtering vs post-filtering,\n",
                "read [Filtering strategies and serving performance](https://blog.vespa.ai/constrained-approximate-nearest-neighbor-search/).\n",
                "[Semantic search with multi-vector indexing](https://blog.vespa.ai/semantic-search-with-multi-vector-indexing/)\n",
                "is a great read overall for this domain.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5bfe782e",
            "metadata": {},
            "source": [
                "## Cleanup\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "8ca46387",
            "metadata": {},
            "outputs": [],
            "source": [
                "vespa_docker.container.stop()\n",
                "vespa_docker.container.remove()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        },
        "vscode": {
            "interpreter": {
                "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
